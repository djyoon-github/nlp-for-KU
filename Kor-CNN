# 필요한 라이브러리 설치
!pip install konlpy
import numpy as np
import pandas as pd
import pickle
import os
import re
import matplotlib.pyplot as plt
%matplotlib inline
from collections import Counter
from time import time
import urllib.request
from konlpy.tag import Okt
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# data set 불러오기

def read_data(filename):
    with open(filename, "r", encoding = "utf-8") as f:
        data = [line.split("\t") for line in f.read().splitlines()]
        data = data[1:]
    
    return data

train_data = read_data("/content/drive/MyDrive/Python/ratings_train.txt")
train_df = pd.DataFrame(train_data) # pandas 사용
test_data = read_data("/content/drive/MyDrive/Python/ratings_test.txt")
test_df = pd.DataFrame(test_data) # pandas 사용

# 갯수 확인하기
print('훈련용 리뷰 개수 :',len(train_df))
print('테스트용 리뷰 개수 :',len(test_df))

# 칼럼명 변경
train_df.columns=['ID','document','label']
test_df.columns=['ID','document','label']

# 데이터처리 (training data)
train_df.drop_duplicates(subset=['document'], inplace=True)
train_df['document'] = train_df['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","") # 한글과 공백을 제외하고 모두 제거
train_df['document'].replace('', np.nan, inplace=True)
train_df = train_df.dropna(how = 'any')

# 데이터처리 (test data)
test_df.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거
test_df['document'] = test_df['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","") # 정규 표현식 수행
test_df['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경
test_df = test_df.dropna(how='any') # Null 값 제거

# 불용어 리스트 구축
stop_words = ['의','가','이','은','들','는','좀','아','것','잘','걍','있','되','과','수','보','주','등','도','를','으로','자','에','와','한','하다','하','이',	'있',	'하',	'것',	'들',	'그',	'되',	'수',	'이',	'보',	'않',	'없',	'나',	'사람',	'주',	'아니',	'등',	'같',	'우리',	'때',	'년',	'가',	'한',	'지',	'대하',	'오',	'말',	'일',	'그렇',	'위하',	'때문',	'그것',	'두',	'말하',	'알',	'그러나',	'받',	'못하',	'일',	'그런',	'또',	'문제',	'더',	'사회',	'많',	'그리고',	'좋',	'크',	'따르',	'중',	'나오',	'가지',	'씨',	'시키',	'만들',	'지금',	'생각하',	'그러',	'속',	'하나',	'집',	'살',	'모르',	'적',	'월',	'데',	'자신',	'안',	'어떤',	'내',	'내',	'경우',	'명',	'생각',	'시간',	'그녀',	'다시',	'이런',	'앞',	'보이',	'번',	'나',	'다른',	'어떻',	'여자',	'개',	'전',	'들',	'사실',	'이렇',	'점',	'싶',	'말',	'정도',	'좀',	'원',	'잘',	'통하',	'소리',	'놓']

# 토큰화-불용어제거-인덱스부여 진행 (training / test)
X_train = []
okt = Okt()
for sentence in train_df['document']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True) # 토큰화
    temp_X = [word for word in temp_X if not word in stop_words] # 불용어 제거
    X_train.append(temp_X)
    
X_test = []
okt = Okt()
for sentence in test_df['document']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True) # 토큰화
    temp_X = [word for word in temp_X if not word in stop_words] # 불용어 제거
    X_test.append(temp_X)
    
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

# 단어의 빈도수 계산(threshold = 3 으로 설정, 희귀 단어 제외하고 vocab 사전 구축)
threshold = 3
total_cnt = len(tokenizer.word_index)
rare_cnt = 0 # 
total_freq = 0 
rare_freq = 0 

for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value
   
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

vocab_size = total_cnt - rare_cnt + 2
print('단어 집합의 크기 :',vocab_size)

tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# null data 제거
drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0) 

# 라벨링 데이터 Array로 묶음(float32형식)
y_train = np.array(train_df['label'],dtype='float32')
y_test = np.array(test_df['label'],dtype='float32')

# 패딩 작업 진행(길이 맞춤, 길이는 25로 설정, max_len)
max_len = 25
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)

# 전처리 완료 후 CNN 모델링 前 필요 라이브러리 불러오기
from keras import models
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras import optimizers
from keras import layers
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers.core import Dense, SpatialDropout1D 
from keras.layers.convolutional import Conv1D 
from keras.layers.embeddings import Embedding
from keras.layers.pooling import GlobalMaxPooling1D

# CNN모델링 생성
EMBED_SIZE = 300 
NUM_FILTERS = 10 
NUM_WORDS = 3

model = Sequential()
model.add(Embedding(vocab_size, EMBED_SIZE, input_length=max_len)) 
model.add(SpatialDropout1D(0.5)) 
model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation="relu")) 
model.add(GlobalMaxPooling1D()) 
model.add(Dense(1, activation="sigmoid")) 

model.summary()

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model_k.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=32, validation_split=0.2)

loaded_model = load_model('best_model_k.h5')

# 시각화
fig, loss_ax = plt.subplots()
acc_ax = loss_ax.twinx()

loss_ax.plot(history.history['loss'], 'y', label='train loss')
loss_ax.plot(history.history['val_loss'], 'r', label='val loss')
loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
loss_ax.legend(loc='upper right')

acc_ax.plot(history.history['acc'], 'b', label='train acc')
acc_ax.plot(history.history['val_acc'], 'g', label='val acc')
acc_ax.set_ylabel('accuracy')
acc_ax.legend(loc='upper left')

plt.show()

#테스트데이터에 대한 검증 진행
print("\n 테스트 정확도: %.4f" % (loaded_model.evaluate(X_test, y_test)[1]))

# Prediction

def sentiment_predict2(new_sentence):
  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화
  new_sentence = [word for word in new_sentence if not word in stop_words] # 불용어 제거
  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩
  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩
  score = float(loaded_model.predict(pad_new)) # 예측
  if(score > 0.5):
    return 1
  else:
    return 0
    
import pandas as pd
data = pd.read_csv("/content/drive/MyDrive/Python/ko_data.csv", sep=",", encoding="utf-8")
data.to_csv("/content/drive/MyDrive/Python/ko_data.csv", sep=",", index=False)
data['Predicted'] = ''

for i in range(0,len(data)): 
  data.loc[i,'Predicted'] = sentiment_predict2(data.loc[i,'Sentence'])
  
data.drop(['Sentence'],axis='columns',inplace=True)

# 예측 데이터 sample 
data.to_csv("/content/drive/MyDrive/Python/sample_kor_9.csv", sep=",", index=False)



